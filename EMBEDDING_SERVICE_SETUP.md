# FastAPI Embedding Service Setup

This guide explains how to set up and use the FastAPI embedding service with the Node.js backend.

## Overview

The FastAPI embedding service provides local embedding generation using sentence-transformers, eliminating the need for OpenAI API calls and reducing costs.

## Prerequisites

- Python 3.9+ (Python 3.14.0 verified)
- pip package manager
- Virtual environment (recommended)

## Setup Steps

### 1. Navigate to FastAPI Service Directory

```bash
cd fastapi-embedding-service
```

### 2. Create Virtual Environment

```bash
python -m venv venv
```

### 3. Activate Virtual Environment

**Windows:**

```bash
venv\Scripts\activate
```

**Linux/Mac:**

```bash
source venv/bin/activate
```

### 4. Install Dependencies

```bash
pip install -r requirements.txt
```

This will install:

- FastAPI
- sentence-transformers
- PyTorch
- And other required packages

**Note:** First installation may take 5-10 minutes as it downloads the embedding model.

### 5. Configure Environment Variables

Copy the example env file and edit:

```bash
copy .env.example .env  # Windows
# or
cp .env.example .env    # Linux/Mac
```

Edit `.env` file:

```env
EMBEDDING_MODEL=all-MiniLM-L6-v2
DEVICE=cpu
API_PORT=8001
```

### 6. Start the FastAPI Service

```bash
python main.py
```

The service will start on `http://localhost:8001`

### 7. Verify Service is Running

Open browser: `http://localhost:8001/docs` to see the interactive API documentation.

Or test health endpoint:

```bash
curl http://localhost:8001/api/v1/health/
```

## Node.js Backend Configuration

### Update .env File

Add these variables to your Node.js backend `.env` file:

```env
# Embedding Provider
EMBEDDING_PROVIDER=fastapi

# FastAPI Embedding Service URL
EMBEDDING_API_URL=http://localhost:8001

# Optional: Timeout (milliseconds)
EMBEDDING_TIMEOUT_MS=30000
```

### Provider Options

- `openai` - Use OpenAI API (default)
- `ollama` - Use Ollama local embeddings
- `fastapi` - Use FastAPI embedding service (recommended)

## Usage

Once configured, the Node.js backend will automatically use the FastAPI embedding service when `EMBEDDING_PROVIDER=fastapi` is set.

### Testing

1. Start FastAPI service:

   ```bash
   cd fastapi-embedding-service
   python main.py
   ```

2. Start Node.js backend:

   ```bash
   cd nodejs-backend
   npm run dev
   ```

3. Create a chatbot and train it - embeddings will be generated by FastAPI service.

## API Endpoints

### Health Check

- `GET /api/v1/health/` - Basic health check
- `GET /api/v1/health/ready` - Readiness check (verifies model loaded)

### Embeddings

- `POST /api/v1/embeddings/generate` - Single embedding
- `POST /api/v1/embeddings/batch` - Batch embeddings
- `GET /api/v1/embeddings/models` - List models

## Model Information

### Default Model: `all-MiniLM-L6-v2`

- **Dimensions:** 384
- **Speed:** Very Fast
- **Size:** ~80 MB
- **Quality:** Good for general use

### Alternative Models

You can change the model in `.env`:

- `all-mpnet-base-v2` - Better quality (768 dimensions, slower)
- `paraphrase-multilingual-MiniLM-L12-v2` - Multilingual support

## Performance

- **First Request:** ~2-5 seconds (model loading)
- **Subsequent Requests:** ~50-200ms per embedding
- **Batch Processing:** Much faster than sequential

## Troubleshooting

### Service Won't Start

- Check Python version: `python --version` (need 3.9+)
- Verify virtual environment is activated
- Check port 8001 is not in use

### Connection Errors

- Verify FastAPI service is running
- Check `EMBEDDING_API_URL` in Node.js `.env`
- Test with: `curl http://localhost:8001/api/v1/health/`

### Model Loading Issues

- First run downloads model automatically
- Check internet connection
- Model size: ~80 MB for default model

### Slow Performance

- Use GPU: Set `DEVICE=cuda` in `.env` (requires CUDA)
- Increase `BATCH_SIZE` for batch processing
- Use faster model (all-MiniLM-L6-v2 is fastest)

## Production Deployment

For production:

1. Use a process manager (PM2, systemd, etc.)
2. Set `DEVICE=cuda` if GPU available
3. Use production WSGI server:
   ```bash
   uvicorn app.main:app --host 0.0.0.0 --port 8001 --workers 4
   ```
4. Configure reverse proxy (nginx)
5. Set appropriate `LOG_LEVEL=INFO` or `WARNING`

## Benefits

✅ **Cost Savings** - No OpenAI API costs  
✅ **Faster** - Local processing, no network latency  
✅ **Privacy** - Data stays on your server  
✅ **Reliable** - No external API dependencies  
✅ **Scalable** - Can handle high volume
